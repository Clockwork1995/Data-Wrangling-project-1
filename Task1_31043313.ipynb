{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1 : Task 1\n",
    "#### Student Name: Md. Saadman Hossain\n",
    "#### Student ID: 31043313\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "        import re\n",
    "        import os\n",
    "        import langid\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "this section of the assesment includes extracting twitter data from text files and exporting it to a xml file. required tasks are:\n",
    "\n",
    "1. Extract id, date and text from 2422 text files\n",
    "2. wrangle data according to specs and export to xml file\n",
    "\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parsing Text Files (%55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import langid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this part i read all the files for part 1 and compiled them into a text file called combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(r'C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\data part 1/') \n",
    "output = '' \n",
    "for file in file_names: \n",
    "    with open('C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\data part 1\\\\'+file, encoding = 'utf8') as f: \n",
    "        content = f.read().strip('\\n') \n",
    "    output += content + '\\t0\\n'   \n",
    "    \n",
    "#encode your string by using .encode()    \n",
    "output = output.encode()\n",
    "with open('CombinedData.txt', 'wb') as f: \n",
    "    f.write(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i searched through the combined data file to extract the ids, created_at and texts using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\'+'CombinedData.txt','r',encoding=\"utf8\").read()\n",
    "\n",
    "#ids\n",
    "ids = re.findall(r'\"id\":\"(\\d{19})\"', file)\n",
    "\n",
    "#created_at\n",
    "\n",
    "dates = re.findall(r'\"created_at\":\"((?:(?:\\d{2})?\\d{2})[-](?:0?[1-9]|1[0-2])[-](?:0[1-9]|[12][0-9]|3[01]))', file)\n",
    "\n",
    "#text\n",
    "\n",
    "texts = re.findall(r'\"text\":\"(.*?)\"',file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this step, i combined all the extractd data in a nested list format, where each nested list contains id text and date for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#combining extracted data\n",
    "\n",
    "id_text_date = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    id_text_date.append([])\n",
    "    id_text_date[i].append(ids[i])\n",
    "    id_text_date[i].append(texts[i])\n",
    "    id_text_date[i].append(dates[i])\n",
    "    \n",
    "print(id_text_date[0:2])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id_text_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i removed the duplicate lists in the nested list to remove repeated tweet instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicates\n",
    "\n",
    "cleaned = list(set(tuple(i) for i in id_text_date)) \n",
    "\n",
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To classify the tweets i used langid package iterating through each list and checking if language of the text is english and appending the english tweets into a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify tweets \n",
    "\n",
    "#get only english tweets\n",
    "\n",
    "liss = []\n",
    "\n",
    "for i in range(len(cleaned)):\n",
    "    if langid.classify(cleaned[i][1])[0] == 'en':\n",
    "        liss.append(cleaned[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(liss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(liss[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langid.classify(id_text_date[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_text_date[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(liss[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A list containing all the unique dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique dates\n",
    "\n",
    "emp=[]\n",
    "for i in liss:\n",
    "    emp.append(i[2])\n",
    "    \n",
    "    \n",
    "ass= list(set(emp))\n",
    "print(ass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping all the tweets of same date as lists of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping tweets of same date as lists of list\n",
    "\n",
    "num_lists = len(ass)\n",
    "lists = []\n",
    "for p in range(num_lists):\n",
    "    lists.append([])\n",
    "\n",
    "\n",
    "liss = list(liss)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(liss)):\n",
    "    for k in range(len(ass)):\n",
    "        if liss[i][2] == ass[k]:\n",
    "            lists[k].append(liss[i])\n",
    "        \n",
    "        \n",
    "print(len(lists[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correct\n",
    "z = 0\n",
    "\n",
    "for i in range(len(lists)):\n",
    "    z+= len(lists[i])\n",
    " \n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a list of number of tweets for each date. this is useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "\n",
    "for i in range(len(lists)):\n",
    "    z.append(len(lists[i]))\n",
    " \n",
    "\n",
    "print(z[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the required XML file and adding all the extracted data done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating xml file\n",
    "\n",
    "\n",
    "\n",
    "f = open(\"31043313.xml\", \"w\",encoding='utf-8')\n",
    "f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "f.write('\\n<data>')\n",
    "\n",
    "for i in range(len(ass)):\n",
    "    k = ass[i]\n",
    "    f.write(('\\n<tweets' + ' ' + 'date=' + '\"'+  k + '\"' + '>'))\n",
    "    \n",
    "    fu = z[i]\n",
    "    \n",
    "    for m in range(fu):\n",
    "        \n",
    "        k = ass[i]\n",
    "        f.write(('\\n<tweet' + ' ' + 'id=' + '\"' + lists[i][m][0] + '\"' + '>'))\n",
    "        \n",
    "        if lists[i][m][2] == k:\n",
    "            f.write(lists[i][m][1])\n",
    "        f.write('\\n</tweet>')\n",
    "    \n",
    "    \n",
    "    f.write('\\n</tweets>')\n",
    "\n",
    "f.write('\\n</data>')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lists))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
