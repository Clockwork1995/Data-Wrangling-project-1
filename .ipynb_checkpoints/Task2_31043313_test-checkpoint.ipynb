{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Pre-Processing (%45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firstly, i decided to read the excel file into a pandas dataframe using the ExcelFile pandas functions. I was able to achieve this by:\n",
    "\n",
    "#### 1. Iterating through all the sheet names and parsing the sheets.\n",
    "#### 2. dropping all columns with nulls for each sheet while appending and drop all rows with nulls at the same time\n",
    "#### 3. used pd.concat to merge all the dataframes, removed repeated text, id, created at columns\n",
    "#### 4. renamed the columns\n",
    "#### 5. reset row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Cientificos #Delincuentes https://t.co/qMuDl8...</td>\n",
       "      <td>1241764966912451072</td>\n",
       "      <td>2020-03-22T16:33:33.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Packed UK concerts amid rising Covid-19 cases ...</td>\n",
       "      <td>1241689884911383040</td>\n",
       "      <td>2020-03-22T11:35:12.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#DiputadasQuerétaro   QUERÉTARO REFERENTE A NI...</td>\n",
       "      <td>1241744180352877056</td>\n",
       "      <td>2020-03-22T15:10:57.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QUE PUTAS YA SON TANTOS, PARCEEEE??? O SEA EN ...</td>\n",
       "      <td>1241595765904080896</td>\n",
       "      <td>2020-03-22T05:21:12.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en misiones no hay casos porque acá tenemos al...</td>\n",
       "      <td>1241752132455476992</td>\n",
       "      <td>2020-03-22T15:42:33.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   id  \\\n",
       "0  #Cientificos #Delincuentes https://t.co/qMuDl8...  1241764966912451072   \n",
       "1  Packed UK concerts amid rising Covid-19 cases ...  1241689884911383040   \n",
       "2  #DiputadasQuerétaro   QUERÉTARO REFERENTE A NI...  1241744180352877056   \n",
       "3  QUE PUTAS YA SON TANTOS, PARCEEEE??? O SEA EN ...  1241595765904080896   \n",
       "4  en misiones no hay casos porque acá tenemos al...  1241752132455476992   \n",
       "\n",
       "                 created_at  \n",
       "0  2020-03-22T16:33:33.000Z  \n",
       "1  2020-03-22T11:35:12.000Z  \n",
       "2  2020-03-22T15:10:57.000Z  \n",
       "3  2020-03-22T05:21:12.000Z  \n",
       "4  2020-03-22T15:42:33.000Z  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading excel file\n",
    "\n",
    "data = pd.ExcelFile('sample.xlsx')\n",
    "df = []\n",
    "for sheet in data.sheet_names:\n",
    "    df.append(data.parse(sheet)\n",
    "                         .dropna(axis = 0, how ='all')\n",
    "                         .dropna(axis = 1, how ='all').T.reset_index(drop=True).T)\n",
    "    \n",
    "df = pd.concat(df)\n",
    "\n",
    "df = df[df[0] != 'text']\n",
    "print(len(df))    \n",
    "    \n",
    "df.columns = ['text', 'id', 'created_at']\n",
    "\n",
    "\n",
    "#reset dataframe index\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es', -25.12564468383789)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(str(df['text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using langid.classify function to remove rows in my dataframe which have text not in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9035\n"
     ]
    }
   ],
   "source": [
    "#classifying the dataframe using langid, keeping only the english tweets\n",
    "\n",
    "#remove non english tweets from the dataframe\n",
    "for i in range(len(df['text'])):\n",
    "    if langid.classify(str(df['text'][i]))[0] != 'en':\n",
    "        df = df.drop([i])\n",
    "        \n",
    "print(len(df))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Packed UK concerts amid rising Covid-19 cases ...</td>\n",
       "      <td>1241689884911383040</td>\n",
       "      <td>2020-03-22T11:35:12.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@derek_adesso Hover over any location in the w...</td>\n",
       "      <td>1241577460895863040</td>\n",
       "      <td>2020-03-22T04:08:28.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/AYPd7erCiB</td>\n",
       "      <td>1241586654541824000</td>\n",
       "      <td>2020-03-22T04:45:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@realDonaldTrump Asshole it's called covid-19</td>\n",
       "      <td>1241583861739311104</td>\n",
       "      <td>2020-03-22T04:33:54.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fiddling While Rome Burns, The Reboot https://...</td>\n",
       "      <td>1241652623004099072</td>\n",
       "      <td>2020-03-22T09:07:08.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   id  \\\n",
       "0  Packed UK concerts amid rising Covid-19 cases ...  1241689884911383040   \n",
       "1  @derek_adesso Hover over any location in the w...  1241577460895863040   \n",
       "2                            https://t.co/AYPd7erCiB  1241586654541824000   \n",
       "3      @realDonaldTrump Asshole it's called covid-19  1241583861739311104   \n",
       "4  Fiddling While Rome Burns, The Reboot https://...  1241652623004099072   \n",
       "\n",
       "                 created_at  \n",
       "0  2020-03-22T11:35:12.000Z  \n",
       "1  2020-03-22T04:08:28.000Z  \n",
       "2  2020-03-22T04:45:00.000Z  \n",
       "3  2020-03-22T04:33:54.000Z  \n",
       "4  2020-03-22T09:07:08.000Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resetting index\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing time from created at column for creating of a key value pair dictionary with dates as keys, and text as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Packed UK concerts amid rising Covid-19 cases ...</td>\n",
       "      <td>1241689884911383040</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@derek_adesso Hover over any location in the w...</td>\n",
       "      <td>1241577460895863040</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/AYPd7erCiB</td>\n",
       "      <td>1241586654541824000</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@realDonaldTrump Asshole it's called covid-19</td>\n",
       "      <td>1241583861739311104</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fiddling While Rome Burns, The Reboot https://...</td>\n",
       "      <td>1241652623004099072</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   id  \\\n",
       "0  Packed UK concerts amid rising Covid-19 cases ...  1241689884911383040   \n",
       "1  @derek_adesso Hover over any location in the w...  1241577460895863040   \n",
       "2                            https://t.co/AYPd7erCiB  1241586654541824000   \n",
       "3      @realDonaldTrump Asshole it's called covid-19  1241583861739311104   \n",
       "4  Fiddling While Rome Burns, The Reboot https://...  1241652623004099072   \n",
       "\n",
       "   created_at  \n",
       "0  2020-03-22  \n",
       "1  2020-03-22  \n",
       "2  2020-03-22  \n",
       "3  2020-03-22  \n",
       "4  2020-03-22  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing time from created at\n",
    "\n",
    "df[['created_at','extra_col']] = df['created_at'].str.split('T',expand=True)\n",
    "df = df.drop(columns = ['extra_col'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dictionary with dates as key and text and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "newdf = df\n",
    "\n",
    "#creating a dictionary with dates as key and text and values\n",
    "dict1 = newdf.groupby('created_at')['text'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "print(len(dict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1093"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict1['2020-03-22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9035\n"
     ]
    }
   ],
   "source": [
    "#checking if dictionary is created well\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in dict1.keys():\n",
    "    k+=len(dict1[i])\n",
    "    \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text in the dictionary and then adding the tokenized words into a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the words\n",
    "\n",
    "tokenized_words = []\n",
    "\n",
    "for items in list(dict1.keys()):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens = tokenizer.tokenize(str(dict1[items]).lower()) # converting all the tokens to lower case\n",
    "    tokenized_words += tokens\n",
    "\n",
    "len(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['packed',\n",
       " 'uk',\n",
       " 'concerts',\n",
       " 'amid',\n",
       " 'rising',\n",
       " 'covid',\n",
       " 'cases',\n",
       " 'shock',\n",
       " 'social',\n",
       " 'media',\n",
       " 'free',\n",
       " 'malaysia',\n",
       " 'today',\n",
       " 'https',\n",
       " 't',\n",
       " 'co',\n",
       " 'r',\n",
       " 'gu',\n",
       " 'wj',\n",
       " 'qy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding top 200 meaningful bigrams using pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding top 200 meaningful bigrams using pmi\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(tokenized_words)\n",
    "\n",
    "finder.apply_freq_filter(20)\n",
    "finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "\n",
    "meaningful_bigrams = finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "# # creating bigram words to add to vocab\n",
    "# pmi_bigrams = []\n",
    "# for word in meaningful_bigrams:\n",
    "#     pmi_bigrams.append('_'.join(word))\n",
    "    \n",
    "    \n",
    "# pmi_bigrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 32027)\n"
     ]
    }
   ],
   "source": [
    "#mwetokenizer\n",
    "\n",
    "kkdict = {}\n",
    "\n",
    "for key in dict1.keys():\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens = tokenizer.tokenize(str(dict1[key]).lower())  #.lower()\n",
    "    kkdict[key] = tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mwetokenizer = MWETokenizer(meaningful_bigrams)\n",
    "# colloc_patents =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in kkdict.items())\n",
    "# all_words_colloc = list(chain.from_iterable(colloc_patents.values()))\n",
    "# colloc_voc = list(set(all_words_colloc))\n",
    "# print(len(colloc_voc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pids = []\n",
    "# patent_words = []\n",
    "# for pid, tokens in colloc_patents.items():\n",
    "#     pids.append(pid)\n",
    "#     txt = ' '.join(tokens)\n",
    "#     patent_words.append(txt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "# tfidf_vectors = tfidf_vectorizer.fit_transform(patent_words)\n",
    "# tfidf_vectors.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "\n",
    "data_features = vectorizer.fit_transform([' '.join(value) for value in kkdict.values()])\n",
    "print (data_features.shape)\n",
    "\n",
    "\n",
    "\n",
    "# vocab2 = vectorizer.get_feature_names()\n",
    "\n",
    "# for word, count in zip(vocab2, data_features.toarray()[0]):\n",
    "#     if count > 0:\n",
    "#         print (word, \":\", count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3abde840d693>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#for each dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0msave_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pids' is not defined"
     ]
    }
   ],
   "source": [
    "save_file = open(\"vectorise_test.txt\", 'w')\n",
    "\n",
    "\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "cx = data_features.tocoo()\n",
    "\n",
    "#for each dates\n",
    "for k in pids:\n",
    "    save_file.write(str(k) + ',')\n",
    "    \n",
    "    for i,j,v in list(itertools.zip_longest(cx.row, cx.col, cx.data)): \n",
    "        if i == pids.index(k):\n",
    "            save_file.write(str(j) + ':' + str(v) + ',')\n",
    "    # removing the extra comma after each iteration\n",
    "    save_file.seek(0, 2)              \n",
    "    save_file.seek(save_file.tell() - 1, 0)  \n",
    "    save_file.truncate()\n",
    "    save_file.write('\\n') # new line for the next date\n",
    "    \n",
    "    \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pmi_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords from vocab\n",
    "        \n",
    "with open(\"stopwords_en.txt\") as stop_words:\n",
    "    stopwords = stop_words.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering the tokens to remove stopwords\n",
    "\n",
    "filtered_tokenized_words = [token for token in tokenized_words if token not in stopwords]\n",
    "\n",
    "len(filtered_tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing tokens with the length less than 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tokens with the length less than 3 \n",
    "\n",
    "len_tokenized_words = [token for token in filtered_tokenized_words if len(token)>=3]\n",
    "\n",
    "len(len_tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove context-dependent (with the threshold set to more than 60 days) stop words\n",
    "\n",
    "\n",
    "\n",
    "#### Here i had to tokenized once again, so that i have a dictionary with dates and tokenized words for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context-dependent (with the threshold set to more than 60 days) stop words must be removed from the vocab\n",
    "\n",
    "\n",
    "#tokenizing by date\n",
    "\n",
    "new_dict = {}\n",
    "\n",
    "for key in dict1.keys():\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens = tokenizer.tokenize(str(dict1[key]).lower())  #.lower()\n",
    "    new_dict[key] = tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if tokenizing worked\n",
    "\n",
    "k=0\n",
    "for i in new_dict.keys():\n",
    "    k+=len(new_dict[i])\n",
    "    \n",
    "\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a dictionary to have only unique tokens for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dict = {}\n",
    "\n",
    "for key in new_dict.keys():\n",
    "    unique = list(set(new_dict[key]))\n",
    "    unique_dict[key] = unique\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if unique dictionary worked\n",
    "\n",
    "k=0\n",
    "for i in unique_dict.keys():\n",
    "    k+=len(unique_dict[i])\n",
    "    \n",
    "\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the unique token words from each day into a list for the calculation of value counts, which will allow me to account for words which appear in more than 60 days or less than 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# putting all the unique words from each day into a list for the calculation of value counts, which will allow me to \n",
    "# account of words which appear in more than 60 days and less than 5 days.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this list has repitions of all unique words in the 81 days\n",
    "unique_list = []\n",
    "\n",
    "for key in unique_dict.keys():\n",
    "    unique_list += unique_dict[key]\n",
    "    \n",
    "    \n",
    "len(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Counter package to make key value pair of unique tokens and their value counts.\n",
    "\n",
    "## Here value counts refer to frequency of days the word appear out of 81 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(unique_list)\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = dict(counts)\n",
    "len(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "for k,v in sorted(counts_dict.items(), key=operator.itemgetter(1), reverse=True)[:5]:\n",
    "    print ('key:'+ str(k) + '|','value:' + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the context dependant words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = dict(counts)\n",
    "\n",
    "\n",
    "# these are the context dependant words in a list\n",
    "\n",
    "context_dep = []\n",
    "\n",
    "for key in counts_dict.keys():\n",
    "    if counts_dict[key]>60:\n",
    "        context_dep.append(key)\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dep[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are rare token words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are rare token words in a list\n",
    "\n",
    "rare_words = []\n",
    "\n",
    "for key in counts_dict.keys():\n",
    "    if counts_dict[key]<5:\n",
    "        rare_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a list of all the unique words in all the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the unique words in all days\n",
    "\n",
    "unique_words = set(unique_list)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing after removing tokens with the length less than 3 step filtering the tokens to remove  context-dependent stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing after removing tokens with the length less than 3 step\n",
    "\n",
    "#filtering the tokens to remove  context-dependent stopwords\n",
    "\n",
    "unique_words = [token for token in unique_words if token not in context_dep]\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the tokens to remove  rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering the tokens to remove  rare tokens\n",
    "\n",
    "unique_words = set(unique_words) - set(rare_words)\n",
    "unique_words = list(unique_words)\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords from vocab\n",
    "        \n",
    "with open(\"stopwords_en.txt\") as stop_words:\n",
    "    stopwords = stop_words.read().splitlines()\n",
    "    \n",
    "#filtering the tokens to remove stopwords\n",
    "\n",
    "unique_words = [token for token in unique_words if token not in stopwords]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing tokens with the length less than 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tokens with the length less than 3 \n",
    "\n",
    "unique_words = [token for token in unique_words if len(token)>=3]\n",
    "unique_words = list(unique_words)\n",
    "len(unique_words)\n",
    "\n",
    "\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming tokens using  Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming tokens using  Porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "unique_words = ['{1}'.format(u, stemmer.stem(u)) for u in unique_words]\n",
    "unique_words = set(unique_words)\n",
    "unique_words = list(unique_words)\n",
    "unique_words.sort()\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the pmi_bigrams to the unique words list from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = unique_words+pmi_bigrams\n",
    "sorted_vocab = sorted(vocab_list)\n",
    "sorted_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning sorted vocab list into a dictionary with index as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {k: v for v, k in enumerate(sorted_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting vocab into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting vocab into a file\n",
    "\n",
    "\n",
    "out_file = open(\"./31043313_vocabgggg.txt\", 'w')\n",
    "\n",
    "for i in vocab_dict.keys():\n",
    "    out_file.write(str(i) + ':' + str(vocab_dict[i]) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram - workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workings for the unigram section\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "#tokenizing by date\n",
    "\n",
    "zdict = {}\n",
    "\n",
    "for key in dict1.keys():\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens = tokenizer.tokenize(str(dict1[key]).lower())  #.lower()\n",
    "    zdict[key] = tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#removing context independant stopwords  \n",
    "\n",
    "for date in zdict.keys():\n",
    "    zdict[date]=[token for token in zdict[date] if token not in stopwords]\n",
    "\n",
    "\n",
    "#stemming\n",
    "for date in zdict.keys():\n",
    "    stem_words = ['{1}'.format(u, stemmer.stem(u)) for u in zdict[date]]\n",
    "    \n",
    "    stem_words = list(stem_words)\n",
    "    stem_words.sort()\n",
    "    zdict[date] = stem_words\n",
    "    \n",
    "#removing words lenth less than 3  \n",
    "for i in zdict.keys():\n",
    "    zdict[i]=[token for token in zdict[i] if len(token)>=3]\n",
    "    \n",
    "    \n",
    "# top 100 most common unigrams    \n",
    "for i in zdict.keys():\n",
    "    zdict[i] = FreqDist(zdict[i]).most_common(100)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdict['2020-03-22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting 100uni into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting 100uni into a file\n",
    "\n",
    "\n",
    "out_file = open(\"./31043313_100unigggg.txt\", 'w')\n",
    "for d in zdict.keys():\n",
    "    out_file.write(''.join(str(zdict[d])).replace('[',(d + ':[')) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram - workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workings for bigram section\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#tokenizing by date\n",
    "\n",
    "bdict = {}\n",
    "\n",
    "for key in dict1.keys():\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens = tokenizer.tokenize(str(dict1[key]).lower())  #.lower()\n",
    "    bdict[key] = tokens\n",
    "\n",
    "\n",
    "# top 100 most common bigrams\n",
    "for i in bdict.keys():\n",
    "    s = ngrams(bdict[i], n = 2)\n",
    "    bdict[i] = FreqDist(s).most_common(100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdict['2020-03-24']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting 100bi into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting 100bi into a file\n",
    "\n",
    "\n",
    "out_file = open(\"./31043313_100bigggg.txt\", 'w')\n",
    "for d in bdict.keys():\n",
    "    out_file.write(''.join(str(bdict[d])).replace('[',(d + ':[')) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
