{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parsing Text Files (%55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import langid\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this part i read all the files for part 1 and compiled them into a text file called combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(r'C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\sample output part 1/') \n",
    "output = '' \n",
    "for file in file_names: \n",
    "    with open('C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\sample output part 1\\\\'+file, encoding = 'utf8') as f: \n",
    "        content = f.read().strip('\\n') \n",
    "    output += content + '\\t0\\n'   \n",
    "    \n",
    "#encode your string by using .encode()    \n",
    "output = output.encode()\n",
    "with open('SampleData.txt', 'wb') as f: \n",
    "    f.write(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i searched through the combined data file to extract the ids, created_at and texts using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:\\\\Users\\\\ClockworK\\\\Desktop\\\\fit5196\\\\Assignment 1\\\\'+'SampleData.txt','r',encoding=\"utf8\").read()\n",
    "\n",
    "#ids\n",
    "ids = re.findall(r'\"id\":\"(\\d{19})\"', file)\n",
    "\n",
    "#created_at\n",
    "\n",
    "dates = re.findall(r'\"created_at\":\"((?:(?:\\d{2})?\\d{2})[-](?:0?[1-9]|1[0-2])[-](?:0[1-9]|[12][0-9]|3[01]))', file)\n",
    "\n",
    "#text\n",
    "\n",
    "texts = re.findall(r'\"text\":\"(.*?)\"',file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n"
     ]
    }
   ],
   "source": [
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18\n"
     ]
    }
   ],
   "source": [
    "print(dates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1251362171998498816', '1251362172627714049', '1251362172732493829', '1251362173432950786', '1251362173705752576']\n"
     ]
    }
   ],
   "source": [
    "print(ids[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we do not follow the lockdown period sincerely, as a result of the COVID-19 infection, India as a nation is bound to fall back 21 years.\\nMaking the right choice is our collective responsibility at this moment. https://t.co/oGhKLLuKxO\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this step, i combined all the extractd data in a nested list format, where each nested list contains id text and date for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1251362171998498816', 'If we do not follow the lockdown period sincerely, as a result of the COVID-19 infection, India as a nation is bound to fall back 21 years.\\\\nMaking the right choice is our collective responsibility at this moment. https://t.co/oGhKLLuKxO', '2020-04-18'], ['1251362172627714049', '@g1rio Se o @wilsonwitzel não estivesse com o coronavirus vcs iam ver ele ir lá pessoalmente pra acabar com essa palhaçada!!!!! ahahahaha #sqn\\\\n#WitzelLixo', '2020-04-18']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#combining extracted data\n",
    "\n",
    "id_text_date = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    id_text_date.append([])\n",
    "    id_text_date[i].append(ids[i])\n",
    "    id_text_date[i].append(texts[i])\n",
    "    id_text_date[i].append(dates[i])\n",
    "    \n",
    "print(id_text_date[0:2])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n"
     ]
    }
   ],
   "source": [
    "print(len(id_text_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i removed the duplicate lists in the nested list to remove repeated tweet instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n"
     ]
    }
   ],
   "source": [
    "#removing duplicates\n",
    "\n",
    "cleaned = list(set(tuple(i) for i in id_text_date)) \n",
    "\n",
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1251362190629597184', '#CSR Donate food, sanitisers, soaps, food, health products, stuff needed to fight #Covid_19 . Do not dump your unsold stocks. @Nestle  @ProcterGamble  @Colgate  @HUL_News  @PatanjaliDairy  @nsitharaman  @RenukaJain6  @ShefVaidya  @Atheist_Krishna  @shrutiahuja110 @ianuragthakur https://t.co/ESjDgTEjqE', '2020-04-18')\n"
     ]
    }
   ],
   "source": [
    "print(cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To classify the tweets i used langid package iterating through each list and checking if language of the text is english and appending the english tweets into a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n"
     ]
    }
   ],
   "source": [
    "#classify tweets \n",
    "\n",
    "#get only english tweets\n",
    "\n",
    "liss = []\n",
    "\n",
    "for i in range(len(cleaned)):\n",
    "    if langid.classify(cleaned[i][1])[0] == 'en':\n",
    "        liss.append(cleaned[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(liss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1251362190629597184', '#CSR Donate food, sanitisers, soaps, food, health products, stuff needed to fight #Covid_19 . Do not dump your unsold stocks. @Nestle  @ProcterGamble  @Colgate  @HUL_News  @PatanjaliDairy  @nsitharaman  @RenukaJain6  @ShefVaidya  @Atheist_Krishna  @shrutiahuja110 @ianuragthakur https://t.co/ESjDgTEjqE', '2020-04-18'), ('1269647490997006336', 'COVID-19 Could See A Declining Trend By Mid-September:\\xa0Report https://t.co/P2EAafJdgM', '2020-06-07'), ('1258367374908104704', 'New report from @MoodysInvSvc demonstrates that additional funding is needed for states to help weather the #COVID19 created budget storm https://t.co/TQx9tFSvGB', '2020-05-07'), ('1258367403895001088', 'As an assassination technique, this goes back to the Great Defenestration of Prague in 1419. Czech tradition and Soviet ambitions seems to have brought the technique into the Russian repertoire with the defenestration of Edvard Beneš in 1948. https://t.co/Ar8deWjki0', '2020-05-07'), ('1251361314473041922', \"It stank a lot when she was bragging she was good friends with Bush Jr, well, now it's dead, rip Ellen, you bad people. https://t.co/PzTRIgijeU\", '2020-04-18'), ('1251362189933494273', 'LOUDER https://t.co/N2ktPU46JJ', '2020-04-18'), ('1260058311900061696', 'Are they at a zoo?\\\\n#OBAMAGATE https://t.co/UTdAVesV3D', '2020-05-12'), ('1258367444785065990', \"All going to die if we don't fix our Climate problem...\\\\ngoing to make #COVID19 look like a walk in the park..\\\\n\\\\nFunnily enough.. we're fucking good at not walking in the park when it means people will die....\\\\n\\\\nso we can adapt to #ClimateChange  with pain but life is worth it\", '2020-05-07'), ('1260058317713530882', 'For Haitians, die of hunger today or coronavirus tomorrow? \\\\nhttps://t.co/v1TKxpmx6A', '2020-05-12'), ('1258367403580325890', 'The historian Paul Fussell once wrote of WWII “Chickenshit can be recognized instantly because it never has anything to do with winning the war.” There are a distressing number of local authorities eager to engage to chickenshit in the #coronavirus war. https://t.co/6fQtluzpdG', '2020-05-07')]\n"
     ]
    }
   ],
   "source": [
    "print(liss[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pt', -214.6796259880066)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langid.classify(id_text_date[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1251362176989900801', 'Sabão ou sabonete, qual o mais eficiente contra o coronavírus? https://t.co/yzrRSLu6AZ https://t.co/4LyprPfcmV', '2020-04-18']\n"
     ]
    }
   ],
   "source": [
    "print(id_text_date[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1251362190629597184', '#CSR Donate food, sanitisers, soaps, food, health products, stuff needed to fight #Covid_19 . Do not dump your unsold stocks. @Nestle  @ProcterGamble  @Colgate  @HUL_News  @PatanjaliDairy  @nsitharaman  @RenukaJain6  @ShefVaidya  @Atheist_Krishna  @shrutiahuja110 @ianuragthakur https://t.co/ESjDgTEjqE', '2020-04-18'), ('1269647490997006336', 'COVID-19 Could See A Declining Trend By Mid-September:\\xa0Report https://t.co/P2EAafJdgM', '2020-06-07'), ('1258367374908104704', 'New report from @MoodysInvSvc demonstrates that additional funding is needed for states to help weather the #COVID19 created budget storm https://t.co/TQx9tFSvGB', '2020-05-07'), ('1258367403895001088', 'As an assassination technique, this goes back to the Great Defenestration of Prague in 1419. Czech tradition and Soviet ambitions seems to have brought the technique into the Russian repertoire with the defenestration of Edvard Beneš in 1948. https://t.co/Ar8deWjki0', '2020-05-07'), ('1251361314473041922', \"It stank a lot when she was bragging she was good friends with Bush Jr, well, now it's dead, rip Ellen, you bad people. https://t.co/PzTRIgijeU\", '2020-04-18'), ('1251362189933494273', 'LOUDER https://t.co/N2ktPU46JJ', '2020-04-18')]\n"
     ]
    }
   ],
   "source": [
    "print(liss[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A list containing all the unique dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-04-18', '2020-05-12', '2020-06-07', '2020-05-07']\n"
     ]
    }
   ],
   "source": [
    "#number of unique dates\n",
    "\n",
    "emp=[]\n",
    "for i in liss:\n",
    "    emp.append(i[2])\n",
    "    \n",
    "    \n",
    "ass= list(set(emp))\n",
    "print(ass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping all the tweets of same date as lists of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# grouping tweets of same date as lists of list\n",
    "\n",
    "num_lists = len(ass)\n",
    "lists = []\n",
    "for p in range(num_lists):\n",
    "    lists.append([])\n",
    "\n",
    "\n",
    "liss = list(liss)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(liss)):\n",
    "    for k in range(len(ass)):\n",
    "        if liss[i][2] == ass[k]:\n",
    "            lists[k].append(liss[i])\n",
    "        \n",
    "        \n",
    "print(len(lists[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n"
     ]
    }
   ],
   "source": [
    "#checking if correct\n",
    "z = 0\n",
    "\n",
    "for i in range(len(lists)):\n",
    "    z+= len(lists[i])\n",
    " \n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a list of number of tweets for each date. this is useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 116, 206, 290]\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in range(len(lists)):\n",
    "    z.append(len(lists[i]))\n",
    " \n",
    "\n",
    "print(z[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the required XML file and adding all the extracted data done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating xml file\n",
    "\n",
    "\n",
    "\n",
    "f = open(\"31043313_test.xml\", \"w\",encoding='utf-8')\n",
    "f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "f.write('\\n<data>')\n",
    "\n",
    "for i in range(len(ass)):\n",
    "    k = ass[i]\n",
    "    f.write(('\\n<tweets' + ' ' + 'date=' + '\"'+  k + '\"' + '>'))\n",
    "    \n",
    "    fu = z[i]\n",
    "    \n",
    "    for m in range(fu):\n",
    "        \n",
    "        k = ass[i]\n",
    "        f.write(('\\n<tweet' + ' ' + 'id=' + '\"' + lists[i][m][0] + '\"' + '>'))\n",
    "        \n",
    "        if lists[i][m][2] == k:\n",
    "            f.write(lists[i][m][1])\n",
    "        f.write('\\n</tweet>')\n",
    "    \n",
    "    \n",
    "    f.write('\\n</tweets>')\n",
    "\n",
    "f.write('\\n</data>')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CSR Donate food, sanitisers, soaps, food, health products, stuff needed to fight #Covid_19 . Do not dump your unsold stocks. @Nestle  @ProcterGamble  @Colgate  @HUL_News  @PatanjaliDairy  @nsitharaman  @RenukaJain6  @ShefVaidya  @Atheist_Krishna  @shrutiahuja110 @ianuragthakur https://t.co/ESjDgTEjqE\n"
     ]
    }
   ],
   "source": [
    "print(lists[0][0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251362190629597184\n"
     ]
    }
   ],
   "source": [
    "print(lists[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough work for task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading excel file\n",
    "\n",
    "# data = pd.ExcelFile('31043313.xlsx')\n",
    "# df = []\n",
    "# for sheet in data.sheet_names:\n",
    "#     df.append(data.parse(sheet)\n",
    "#                          .dropna(axis = 0, how ='all')\n",
    "#                          .dropna(axis = 1, how ='all').T.reset_index(drop=True).T)\n",
    "    \n",
    "# df = pd.concat(df)\n",
    "\n",
    "# df = df[df[0] != 'text']\n",
    "# print(len(df))    \n",
    "    \n",
    "# df.columns = ['text', 'id', 'created_at']\n",
    "\n",
    "\n",
    "# #reset dataframe index\n",
    "\n",
    "# df = df.reset_index(drop=True)\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classifying the dataframe using langid, keeping only the english tweets\n",
    "\n",
    "# df = df.head(50)\n",
    "# for i in range(len(df['text'])):\n",
    "#     if langid.classify(str(df['text'][i]))[0] != 'en':\n",
    "#         df=df.drop([i])\n",
    "        \n",
    "# print(len(df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['created_at','k']] = df['created_at'].str.split('T',expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns = ['k'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = df.groupby('created_at')['id'].apply(list).to_dict()\n",
    "# d2 = df.groupby('created_at')['text'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "# print(d2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #newlist = [word for line in d2['2020-03-22'] for word in line.split()]\n",
    "# tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "# tokens = tokenizer.tokenize(str(d2['2020-03-22']))\n",
    "\n",
    "# len(tokens)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# newlist = [word for line in d2['2020-03-22'] for word in line.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-80d448635803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnewlist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'newlist' is not defined"
     ]
    }
   ],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
